---
title: "17 蒙特卡洛法与自助法"
author: "李世纪"
format: html
editor: visual
---

## 本章导读

在传统计量经济学中，参数的统计推断主要依赖于大样本渐近理论。然而，在实际应用中，研究者常常面临小样本、模型设定复杂、分布假设不满足等问题，此时传统渐近理论的适用性受到限制。随着计算能力的飞速发展，基于计算机模拟的统计方法已成为现代计量经济学不可或缺的工具。

本章系统介绍两类核心的模拟方法：蒙特卡洛法和自助法。蒙特卡洛法通过随机抽样解决确定性计算问题，特别适用于高维积分和复杂期望的计算。自助法则通过重抽样技术，仅基于观测数据即可构造统计量的经验分布，为统计推断提供了一种灵活的数据驱动方法。此外，本章还将深入探讨马尔可夫链蒙特卡洛方法及其变体，这些方法在贝叶斯计量经济学和高维模型估计中发挥着关键作用。

本章的学习目标是：理解各类模拟方法的基本原理；掌握它们在计量经济学中的应用场景；能够根据研究问题选择合适的方法；并正确解释模拟结果。通过本章的学习，读者将获得一套强大的工具，用于处理传统方法难以解决的复杂计量问题。

## 16.1 引言：模拟方法在计量经济学中的作用

### 16.1.1 传统方法的局限性

传统计量经济学推断主要建立在渐近理论基础之上。考虑线性回归模型：

$$
y_i = \mathbf{x}_i'\pmb{\beta} + \varepsilon_i, \quad i=1,\dots,n
$$

其中$\varepsilon_i \sim i.i.d.(0,\sigma^2)$。普通最小二乘估计量$\hat{\pmb{\beta}} = (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y}$在满足经典假设下具有良好性质：无偏性、一致性，且渐近服从正态分布：

$$
\sqrt{n}(\hat{\pmb{\beta}} - \pmb{\beta}) \xrightarrow{d} N(0, \sigma^2 \mathbf{Q}^{-1})
$$

其中$\mathbf{Q} = \text{plim}(n^{-1}\mathbf{X}'\mathbf{X})$。基于此渐近分布，我们可以构造置信区间和假设检验。

然而，这种渐近推断在实际应用中面临诸多挑战：

1.  **小样本问题**：当样本量有限时，渐近近似可能不准确，特别在模型非线性或存在弱工具变量时
2.  **分布假设的敏感性**：许多传统方法对误差项分布有严格要求（如正态性），而实际数据常违反这些假设
3.  **复杂统计量的分布**：对于中位数、分位数、最大似然估计量等复杂统计量，其精确分布难以推导
4.  **模型不确定性**：模型设定误差对传统推断方法的影响难以量化

### 16.1.2 计算机模拟的优势

模拟方法通过计算机生成人工数据来研究统计量的性质，主要优势体现在：

1.  **有限样本性质研究**：直接评估统计量在有限样本下的表现，不依赖于大样本近似
2.  **分布自由**：无需对数据分布做出严格假设
3.  **灵活性**：适用于各种复杂模型和估计方法
4.  **直观性**：通过可视化的方式展示统计量的抽样分布

模拟方法可分为两大类：**基于设计的蒙特卡洛研究**和**基于数据的自助法**。前者需要设定数据生成过程，主要用于方法评估和比较；后者直接基于观测数据，主要用于实际数据的统计推断。

### 16.1.3 方法分类概览

本章将系统介绍以下三类核心方法：

1.  **经典蒙特卡洛法**：基于已知概率分布的随机抽样，用于计算积分、期望和复杂统计量的性质
2.  **自助法**：通过有放回重抽样从原始数据中生成伪样本，用于估计统计量的抽样分布
3.  **马尔可夫链蒙特卡洛**：通过构造马尔可夫链从复杂目标分布中抽样，特别适用于贝叶斯推断

这些方法共同构成了现代计量经济学家的工具箱，极大地扩展了我们处理复杂问题的能力。

## 16.2 蒙特卡洛方法基础

### 16.2.1 蒙特卡洛方法的基本原理

蒙特卡洛方法的核心思想是利用随机抽样解决确定性计算问题。其数学基础是大数定律和中心极限定理。

**定义 16.1 (蒙特卡洛积分)** 考虑计算期望值$\mu = E[g(X)]$，其中$X$是随机变量，$g(\cdot)$是已知函数。若能从$X$的分布$F_X$中生成独立同分布的样本$x_1, x_2, \dots, x_N$，则蒙特卡洛估计量为：

$$
\hat{\mu}_N = \frac{1}{N} \sum_{i=1}^N g(x_i)
$$

由强大数定律，$\hat{\mu}_N \xrightarrow{a.s.} \mu$。由中心极限定理：

$$
\sqrt{N}(\hat{\mu}_N - \mu) \xrightarrow{d} N(0, \sigma^2_g)
$$

其中$\sigma^2_g = \text{Var}[g(X)]$。

蒙特卡洛误差的标准差为$\sigma_g/\sqrt{N}$，以$O(N^{-1/2})$的速度收敛，这一收敛速率与维度无关，使其在高维积分中特别有优势。

### 16.2.2 简单蒙特卡洛积分

考虑一般形式的积分问题：

$$
I = \int_{\mathcal{D}} f(\mathbf{x}) d\mathbf{x}
$$

其中$\mathcal{D} \subseteq \mathbb{R}^d$。若能将积分改写为期望形式，即可应用蒙特卡洛方法。

**例 16.1 (概率计算)** 设$\mathbf{X} \sim p(\mathbf{x})$，要计算$P(\mathbf{X} \in A) = \int_A p(\mathbf{x}) d\mathbf{x}$。定义示性函数$I_A(\mathbf{x}) = 1$当$\mathbf{x} \in A$，否则为0。则：

$$
P(\mathbf{X} \in A) = E[I_A(\mathbf{X})] \approx \frac{1}{N} \sum_{i=1}^N I_A(\mathbf{x}_i)
$$

其中$\mathbf{x}_i \sim p(\mathbf{x})$。

### 16.2.3 重要性抽样

当从目标分布$p(\mathbf{x})$直接抽样困难时，重要性抽样是一种有效的方差缩减技术。

**算法 16.1 (重要性抽样)** 1. 选择提议分布$q(\mathbf{x})$，使其满足：当$p(\mathbf{x}) > 0$时，$q(\mathbf{x}) > 0$ 2. 从$q(\mathbf{x})$中生成独立样本$\mathbf{x}_1, \dots, \mathbf{x}_N$ 3. 计算重要性权重$w_i = p(\mathbf{x}_i)/q(\mathbf{x}_i)$ 4. 估计期望值：$E_p[g(\mathbf{X})] \approx \frac{\sum_{i=1}^N w_i g(\mathbf{x}_i)}{\sum_{i=1}^N w_i}$

归一化权重估计量是渐近无偏的。最优提议分布为$q^*(\mathbf{x}) \propto |g(\mathbf{x})|p(\mathbf{x})$，可使方差最小化。

### 16.2.4 反变换法与接受-拒绝法

**反变换法**适用于一维分布。若$F(x)$是累积分布函数，$U \sim U(0,1)$，则$X = F^{-1}(U)$的分布函数为$F$。

**接受-拒绝法**适用于已知分布密度$p(x)$但难以直接抽样的情况。算法步骤如下：

1.  找到包络函数$Cq(x)$满足$Cq(x) \geq p(x)$对所有$x$
2.  从$q(x)$生成候选样本$x^*$
3.  生成$u \sim U(0,1)$
4.  若$u \leq p(x^*)/[Cq(x^*)]$，则接受$x^*$；否则拒绝

接受概率为$1/C$，效率取决于包络函数的紧致性。

### 16.2.5 蒙特卡洛方法在计量经济学中的应用

在计量经济学中，蒙特卡洛方法主要有三个应用方向：

1.  **有限样本性质研究**：评估估计量在小样本下的偏误、方差和分布形态

2.  **检验功效分析**：计算假设检验在不同备择假设下的拒绝概率

3.  **模型比较与选择**：通过模拟比较不同模型的预测性能

**例 16.2 (工具变量法的有限样本偏误)** 考虑模型：

$$
y_i = \beta x_i + u_i, \quad x_i = \pi z_i + v_i
$$

其中$(u_i, v_i) \sim N(0, \Sigma)$。工具变量估计量为$\hat{\beta}_{IV} = (\mathbf{z}'\mathbf{x})^{-1}\mathbf{z}'\mathbf{y}$。通过蒙特卡洛模拟可以研究： - 弱工具变量($\pi \approx 0$)下的估计量偏误 - 有限样本分布与渐近正态近似的差异 - 不同识别强度下的检验水平扭曲

## 16.3 自助法

### 16.3.1 自助法的基本思想

自助法（bootstrapping）由Efron(1979)提出，其核心是通过对原始样本的有放回重抽样来近似统计量的抽样分布。

设$\mathbf{X}_n = (X_1, \dots, X_n)$是来自分布$F$的独立同分布样本，$\theta = \theta(F)$是感兴趣的参数，$\hat{\theta} = s(\mathbf{X}_n)$是其估计量。我们关心$\hat{\theta}$的抽样分布$G_n(x) = P_F(\hat{\theta} \leq x)$。

**算法 16.2 (非参数自助法)** 1. 从原始样本$\mathbf{X}_n$中有放回地抽取$n$个观测，得到自助样本$\mathbf{X}_n^{*1}$ 2. 计算自助统计量$\hat{\theta}^{*1} = s(\mathbf{X}_n^{*1})$ 3. 重复步骤1-2共$B$次，得到$\hat{\theta}^{*1}, \dots, \hat{\theta}^{*B}$ 4. 用$(\hat{\theta}^{*b}\,(b=1,\ldots,B))$的经验分布近似$\hat{\theta}$的抽样分布

自助法有效性的理论基础是**经验过程理论**。经验分布函数$F_n$以速率$O_p(n^{-1/2})$收敛于真实分布$F$，因此当$s(\cdot)$是平滑函数时，$G_n^*$（基于$F_n$的分布）能很好地近似$G_n$（基于$F$的分布）。

### 16.3.2 非参数自助法

非参数自助法不对总体分布$F$做任何参数假设，直接使用经验分布函数$F_n$作为$F$的估计。

**标准误的自助估计**：

$$
\hat{\text{se}}_B = \sqrt{\frac{1}{B-1} \sum_{b=1}^B (\hat{\theta}^{*b} - \bar{\theta}^*)^2}
$$

其中$\bar{\theta}^* = \frac{1}{B} \sum_{b=1}^B \hat{\theta}^{*b}$。

**百分位置信区间**：令$\hat{\theta}^{*}_{(\alpha)}$表示自助统计量的$\alpha$分位数，则$100(1-\alpha)\%$置信区间为：

$$
[\hat{\theta}^{*}_{(\alpha/2)}, \hat{\theta}^{*}_{(1-\alpha/2)}]
$$

### 16.3.3 参数自助法

当对总体分布有参数假设$F = F_{\pmb{\phi}}$时，可使用参数自助法：

1.  基于原始样本估计参数$\hat{\pmb{\phi}}$
2.  从$F_{\hat{\pmb{\phi}}}$中生成$B$组样本
3.  对每组样本计算$\hat{\theta}^{*b}$

参数自助法在模型假设正确时效率更高，但对模型误设更敏感。

### 16.3.4 各种自助法变体

**残差自助法**：适用于回归模型$y_i = \mathbf{x}_i'\pmb{\beta} + \varepsilon_i$ 1. 拟合模型得到残差$\hat{\varepsilon}_i$和参数估计$\hat{\pmb{\beta}}$ 2. 从$\{\hat{\varepsilon}_1, \dots, \hat{\varepsilon}_n\}$中有放回抽样得到$\varepsilon_i^*$ 3. 生成$y_i^* = \mathbf{x}_i'\hat{\pmb{\beta}} + \varepsilon_i^*$ 4. 重新估计模型得到$\hat{\pmb{\beta}}^*$

**块自助法**：针对时间序列数据的依赖性 1. 将时间序列划分为长度为$l$的重叠块：$B_1 = (y_1, \dots, y_l)$, $B_2 = (y_2, \dots, y_{l+1})$, ... 2. 从这些块中有放回抽样，拼接成自助样本 3. 最优块长$l = O(n^{1/3})$，由数据依赖性决定

**对偶自助法**：适用于异方差模型 生成$y_i^* = \mathbf{x}_i'\hat{\pmb{\beta}} + \hat{\varepsilon}_i v_i^*$，其中$v_i^*$独立同分布，满足$E[v_i^*]=0$, $\text{Var}[v_i^*]=1$

### 16.3.5 自助法的统计性质

**定理 16.1 (自助法的一致性)** 设$\hat{\theta}_n$是$\theta$的估计量，若： 1. $\sqrt{n}(\hat{\theta}_n - \theta) \xrightarrow{d} N(0, \sigma^2)$ 2. $\hat{\sigma}_n^2 \xrightarrow{p} \sigma^2$ 3. $s(\cdot)$在适当意义下平滑

则自助分布一致地近似真实抽样分布：

$$
\sup_x |P_*(\sqrt{n}(\hat{\theta}_n^* - \hat{\theta}_n) \leq x) - P(\sqrt{n}(\hat{\theta}_n - \theta) \leq x)| \xrightarrow{p} 0
$$

其中$P_*$表示给定原始样本的条件概率。

自助法常能提供**高阶准确性**。对于平滑函数模型，自助置信区间有覆盖误差$O(n^{-1})$，而基于正态近似的区间仅有$O(n^{-1/2})$的覆盖误差。

### 16.3.6 计量经济学应用

**异方差稳健推断**：在存在异方差时，传统OLS标准误失效。自助法（特别是对偶自助法）能提供有效的推断。

**工具变量法**：在弱工具变量情况下，传统检验水平扭曲严重。自助Anderson-Rubin检验能提供更可靠的推断。

**分位数回归**：分位数估计量的渐近方差涉及密度估计，计算复杂。自助法直接提供标准误和置信区间。

**模型选择**：通过自助法估计模型预测误差，用于比较不同模型的样本外预测能力。

## 16.4 马尔可夫链蒙特卡洛方法

### 16.4.1 MCMC的基本框架

MCMC用于从复杂目标分布$\pi(\mathbf{x})$中生成样本，其中$\mathbf{x} \in \mathcal{X} \subseteq \mathbb{R}^d$。基本思想是构造一个马尔可夫链，使其平稳分布等于目标分布。

**定义 16.2 (马尔可夫链)** 随机序列${\mathbf{X}^{(t)}}_{t=0}^\infty$称为马尔可夫链，若满足：

$$
P(\mathbf{X}^{(t+1)} \in A | \mathbf{X}^{(t)} = \mathbf{x}^{(t)}, \dots, \mathbf{X}^{(0)} = \mathbf{x}^{(0)}) = P(\mathbf{X}^{(t+1)} \in A | \mathbf{X}^{(t)} = \mathbf{x}^{(t)})
$$

转移核$P(\mathbf{x}, A) = P(\mathbf{X}^{(t+1)} \in A | \mathbf{X}^{(t)} = \mathbf{x})$完全刻画了链的演化。

**定义 16.3 (平稳分布)** 分布$\pi$称为转移核$P$的平稳分布，若满足：

$$
\pi(A) = \int_{\mathcal{X}} P(\mathbf{x}, A) \pi(\mathbf{x}) d\mathbf{x}, \quad \forall A \subseteq \mathcal{X}
$$

**细致平衡条件**是充分条件：若存在转移核$p(\mathbf{x}, \mathbf{y})$满足：

$$
\pi(\mathbf{x}) p(\mathbf{x}, \mathbf{y}) = \pi(\mathbf{y}) p(\mathbf{y}, \mathbf{x}), \quad \forall \mathbf{x}, \mathbf{y} \in \mathcal{X}
$$

则$\pi$是平稳分布。

### 16.4.2 Metropolis-Hastings算法

**算法 16.3 (Metropolis-Hastings)** 给定当前状态$\mathbf{x}^{(t)}$： 1. 从提议分布$q(\cdot | \mathbf{x}^{(t)})$生成候选值$\mathbf{x}^*$ 2. 计算接受概率：

$$
\alpha(\mathbf{x}^{(t)}, \mathbf{x}^*) = \min\left\{1, \frac{\pi(\mathbf{x}^*) q(\mathbf{x}^{(t)} | \mathbf{x}^*)}{\pi(\mathbf{x}^{(t)}) q(\mathbf{x}^* | \mathbf{x}^{(t)})}\right\}
$$

3.  以概率$\alpha$接受$\mathbf{x}^{(t+1)} = \mathbf{x}^*$，否则$\mathbf{x}^{(t+1)} = \mathbf{x}^{(t)}$

MH算法的转移核为：

$$
p_{\text{MH}}(\mathbf{x}, \mathbf{y}) = q(\mathbf{y}|\mathbf{x})\alpha(\mathbf{x}, \mathbf{y}) + \delta_{\mathbf{x}}(\mathbf{y})\left[1 - \int q(\mathbf{z}|\mathbf{x})\alpha(\mathbf{x}, \mathbf{z}) d\mathbf{z}\right]
$$

满足细致平衡条件，因此$\pi$是平稳分布。

**随机游走MH**：取$q(\mathbf{y}|\mathbf{x}) = f(\mathbf{y} - \mathbf{x})$，其中$f$是对称密度（如多元正态）。此时接受概率简化为：

$$
\alpha = \min\left\{1, \frac{\pi(\mathbf{y})}{\pi(\mathbf{x})}\right\}
$$

### 16.4.3 Gibbs抽样

Gibbs抽样适用于高维分布，当满条件分布易于抽样时特别高效。

**算法 16.4 (Gibbs抽样)** 设$\mathbf{x} = (x_1, \dots, x_d)$，目标分布为$\pi(\mathbf{x})$： 1. 初始化$\mathbf{x}^{(0)} = (x_1^{(0)}, \dots, x_d^{(0)})$ 2. 对$t=0,1,\dots$，依次更新： $$
   \begin{aligned}
   x_1^{(t+1)} &\sim \pi(x_1 | x_2^{(t)}, \dots, x_d^{(t)}) \\
   x_2^{(t+1)} &\sim \pi(x_2 | x_1^{(t+1)}, x_3^{(t)}, \dots, x_d^{(t)}) \\
   &\vdots \\
   x_d^{(t+1)} &\sim \pi(x_d | x_1^{(t+1)}, \dots, x_{d-1}^{(t+1)})
   \end{aligned}
   $$

Gibbs抽样是MH算法的特例，其中提议分布取为满条件分布，接受概率恒为1。

**定理 16.2 (Gibbs抽样的收敛性)** 若满条件分布几乎处处正，则Gibbs链不可约、非周期，且以$\pi$为唯一平稳分布。

### 16.4.4 MCMC的收敛诊断

MCMC产生的样本序列是相关的，需要判断链是否收敛到平稳分布。

**燃烧期**：丢弃链的初始部分，消除初始值影响。通常丢弃前$10-50\%$的迭代。

**自相关分析**：计算样本自相关系数：

$$
\hat{\rho}_k = \frac{\sum_{t=1}^{T-k} (x^{(t)} - \bar{x})(x^{(t+k)} - \bar{x})}{\sum_{t=1}^T (x^{(t)} - \bar{x})^2}
$$

高自相关意味着有效样本量减少。

**Gelman-Rubin统计量**：运行$m$条独立链，每条链长度$2T$，丢弃前半部分。定义： - 链内方差：$W = \frac{1}{m}\sum_{j=1}^m s_j^2$ - 链间方差：$B = \frac{T}{m-1}\sum_{j=1}^m (\bar{x}_j - \bar{x})^2$ - 合并方差估计：$\hat{V} = \frac{T-1}{T}W + \frac{1}{T}B$

统计量$\hat{R} = \sqrt{\hat{V}/W}$，当$\hat{R} \approx 1$时表明收敛。

### 16.4.5 贝叶斯计量经济学中的MCMC

在贝叶斯框架下，参数$\pmb{\theta}$的后验分布为：

$$
\pi(\pmb{\theta}|\mathbf{y}) \propto L(\mathbf{y}|\pmb{\theta}) p(\pmb{\theta})
$$

其中$L$是似然函数，$p$是先验分布。MCMC用于从后验分布抽样。

**例 16.3 (贝叶斯线性回归)** 模型：$\mathbf{y} \sim N(\mathbf{X}\pmb{\beta}, \sigma^2\mathbf{I})$ 先验：$\pmb{\beta} \sim N(\pmb{\beta}_0, \mathbf{V}_0)$，$\sigma^2 \sim \text{IG}(a_0, b_0)$

满条件分布： $$
\begin{aligned}
\pmb{\beta}|\sigma^2, \mathbf{y} &\sim N(\pmb{\beta}_n, \mathbf{V}_n) \\
\sigma^2|\pmb{\beta}, \mathbf{y} &\sim \text{IG}(a_n, b_n)
\end{aligned}
$$

其中： $$
\begin{aligned}
\mathbf{V}_n^{-1} &= \mathbf{V}_0^{-1} + \frac{1}{\sigma^2}\mathbf{X}'\mathbf{X} \\
\pmb{\beta}_n &= \mathbf{V}_n\left(\mathbf{V}_0^{-1}\pmb{\beta}_0 + \frac{1}{\sigma^2}\mathbf{X}'\mathbf{y}\right) \\
a_n &= a_0 + \frac{n}{2} \\
b_n &= b_0 + \frac{1}{2}(\mathbf{y} - \mathbf{X}\pmb{\beta})'(\mathbf{y} - \mathbf{X}\pmb{\beta})
\end{aligned}
$$

可直接应用Gibbs抽样。

## 16.5 高级MCMC方法

### 16.5.1 哈密尔顿蒙特卡洛

HMC结合了物理系统的哈密尔顿动力学，能有效探索高维参数空间。

考虑扩展状态空间$(\mathbf{q}, \mathbf{p})$，其中$\mathbf{q}$是位置变量（感兴趣参数），$\mathbf{p}$是动量变量。定义哈密尔顿函数：

$$
H(\mathbf{q}, \mathbf{p}) = U(\mathbf{q}) + K(\mathbf{p})
$$

其中$U(\mathbf{q}) = -\log \pi(\mathbf{q})$是势能，$K(\mathbf{p}) = \frac{1}{2}\mathbf{p}'\mathbf{M}^{-1}\mathbf{p}$是动能，$\mathbf{M}$是质量矩阵。

哈密尔顿方程： $$
\begin{aligned}
\frac{d\mathbf{q}}{dt} &= \frac{\partial H}{\partial \mathbf{p}} = \mathbf{M}^{-1}\mathbf{p} \\
\frac{d\mathbf{p}}{dt} &= -\frac{\partial H}{\partial \mathbf{q}} = -\nabla U(\mathbf{q})
\end{aligned}
$$

**算法 16.5 (蛙跳算法)** 给定当前状态$(\mathbf{q}, \mathbf{p})$和步长$\varepsilon$、步数$L$： 1. 动量刷新：$\mathbf{p} \sim N(0, \mathbf{M})$ 2. 蛙跳积分：对$l=1,\dots,L$ $$
   \begin{aligned}
   \mathbf{p} &\leftarrow \mathbf{p} - \frac{\varepsilon}{2} \nabla U(\mathbf{q}) \\
   \mathbf{q} &\leftarrow \mathbf{q} + \varepsilon \mathbf{M}^{-1}\mathbf{p} \\
   \mathbf{p} &\leftarrow \mathbf{p} - \frac{\varepsilon}{2} \nabla U(\mathbf{q})
   \end{aligned}
   $$ 3. 以概率$\min{1, \exp(-H(\mathbf{q}^*, \mathbf{p}^*) + H(\mathbf{q}, \mathbf{p}))}$接受$(\mathbf{q}^*, -\mathbf{p}^*)$

HMC的关键优势是能产生远离当前状态的提议，接受率高，特别适用于高维、相关参数。

### 16.5.2 No-U-Turn采样器

NUTS是HMC的自适应变体，自动调整步长$\varepsilon$和步数$L$。

核心思想是通过递归构建二叉树，当轨迹开始"回转"（新位置与初始位置的点积为负）时停止模拟。算法自动确定最优积分时间，避免手动调参。

### 16.5.3 切片抽样

切片抽样通过引入辅助变量实现从目标分布的抽样。

**算法 16.6 (切片抽样)** 目标分布$\pi(x) \propto f(x)$： 1. 给定当前$x$，在$[0, f(x)]$上均匀抽取$u$ 2. 从水平集${x': f(x') \geq u}$中均匀抽取新$x'$

水平集可通过 stepping-out 和 shrinkage 方法有效抽样。

### 16.5.4 可逆跳MCMC

RJ-MCMC用于模型选择问题，允许在不同维度的参数空间间跳跃。

设模型$M_k$有参数$\pmb{\theta}_k \in \mathbb{R}^{d_k}$，后验概率为$p(M_k, \pmb{\theta}_k|\mathbf{y})$。从模型$M_k$跳转到$M_{k'}$时，需要维度匹配：引入随机向量$\mathbf{u} \sim q(\mathbf{u})$和$\mathbf{u}' \sim q'(\mathbf{u}')$，建立双射：

$$
(\pmb{\theta}_{k'}, \mathbf{u}') = g_{k\to k'}(\pmb{\theta}_k, \mathbf{u})
$$

接受概率为：

$$
\alpha = \min\left\{1, \frac{p(M_{k'}, \pmb{\theta}_{k'}|\mathbf{y})}{p(M_k, \pmb{\theta}_k|\mathbf{y})} \frac{q'(\mathbf{u}')}{q(\mathbf{u})} \left|\frac{\partial g_{k\to k'}(\pmb{\theta}_k, \mathbf{u})}{\partial (\pmb{\theta}_k, \mathbf{u})}\right|\right\}
$$

## 16.6 方法比较与选择

### 16.6.1 自助法 vs 蒙特卡洛法 vs MCMC

| 特性         | 经典蒙特卡洛       | 自助法             | MCMC                 |
|------------------|-------------------|------------------|------------------|
| **数据要求** | 已知分布假设       | 仅需观测数据       | 已知分布形式         |
| **计算目标** | 积分/期望计算      | 抽样分布近似       | 复杂分布抽样         |
| **样本性质** | 独立同分布         | 近似独立           | 序列相关             |
| **收敛速度** | $O(N^{-1/2})$      | $O(n^{-1})$ (高阶) | 依赖混合时间         |
| **主要应用** | 方法评估、模拟研究 | 频率推断、置信区间 | 贝叶斯推断、高维问题 |
| **实现难度** | 低                 | 中                 | 高                   |

### 16.6.2 小样本性能

在小样本情况下，不同方法的表现差异显著：

1.  **自助法**：当$n < 50$时，非参数自助法可能不稳定，特别是对于非平滑统计量
2.  **参数自助法**：在模型正确设定下表现良好，但对模型误设敏感
3.  **MCMC**：小样本下后验分布可能受先验影响大，需要谨慎选择先验
4.  **贝叶斯自助法**：结合自助法与贝叶斯思想，为小样本推断提供稳健方法

### 16.6.3 计算成本考量

选择方法时需权衡计算成本与统计效率：

1.  **时间复杂度**：

    -   自助法：$O(B \cdot C(n))$，其中$C(n)$是估计量计算成本
    -   MCMC：$O(T \cdot C_{\text{iter}})$，$T$为迭代次数，$C_{\text{iter}}$为单次迭代成本

2.  **存储需求**：MCMC需要存储完整链，内存需求随维度线性增长

3.  **并行化潜力**：

    -   自助法：天然并行，不同自助样本可独立计算
    -   MCMC：序列相关限制了并行化，但可运行多条独立链

4.  **收敛验证**：MCMC需要诊断收敛，增加了计算和人力成本

**实践建议**： - 对于简单模型的频率推断，优先考虑自助法 - 对于高维贝叶斯模型，MCMC是必要工具 - 当计算资源有限时，考虑重要性抽样等方差缩减技术 - 始终进行敏感性分析，检验方法选择对结论的影响

### **16.7 综合案例分析**

-   **16.7.1 案例一：线性回归模型的稳健推断**

    -   问题背景：存在异方差时的OLS推断

    -   方法应用：残差自助法 vs 对偶自助法

    -   实现步骤：R/Python代码演示

    -   结果比较：与传统稳健标准误的对比

-   **16.7.2 案例二：时间序列模型的自助推断**

    -   问题背景：ARMA模型参数的不确定性

    -   方法应用：块自助法实现

    -   关键问题：块长度的选择

    -   应用扩展：预测区间构造

-   **16.7.3 案例三：贝叶斯逻辑回归的MCMC估计**

    -   模型设定：二元选择模型

    -   方法选择：Metropolis-Hastings vs Gibbs抽样

    -   实现细节：先验选择与收敛诊断

    -   结果解释：后验分布与可信区间

-   **16.7.4 案例四：高维VAR模型的哈密尔顿蒙特卡洛**

    -   问题挑战：宏观经济VAR的参数估计

    -   方法优势：HMC在高维空间的效率

    -   实施步骤：Stan/PyMC3实现

    -   经济解释：脉冲响应函数的不确定性

-   **16.7.5 案例五：工具变量法的自助法检验**

    -   问题背景：弱工具变量问题

    -   方法应用：自助法Anderson-Rubin检验

    -   比较分析：与传统检验方法的优劣

    -   实践建议：实际研究中的应用指南

## 本章小结

本章系统介绍了三类核心的模拟方法：蒙特卡洛法、自助法和MCMC。这些方法为现代计量经济学提供了强大的计算工具，极大地扩展了我们处理复杂问题的能力。

**关键要点总结**：

1.  **蒙特卡洛法**是基于随机抽样的数值计算方法，其核心是利用大数定律和中心极限定理。重要性抽样等方差缩减技术能显著提高计算效率。

2.  **自助法**通过重抽样技术，仅基于观测数据即可近似统计量的抽样分布。非参数自助法灵活稳健，参数自助法在模型正确时效率更高，各种变体（块自助法、对偶自助法等）适应不同数据结构。

3.  **MCMC方法**通过构造马尔可夫链从复杂分布中抽样。Metropolis-Hastings算法是最一般的形式，Gibbs抽样在满条件分布易于抽样时高效，哈密尔顿蒙特卡洛特别适合高维问题。

4.  **方法选择**需要综合考虑问题性质、数据特征、计算资源和统计目标。自助法适合频率推断，MCMC适合贝叶斯分析，经典蒙特卡洛适合方法评估。

**未来发展方向**： - 大数据场景下的高效算法 - 深度学习与模拟方法的结合 - 不确定性量化的新方法 - 自动化模型选择与推断

模拟方法已成为现代计量经济学不可或缺的部分。掌握这些工具不仅有助于解决传统方法难以处理的问题，也为探索新的方法论提供了可能。在实际应用中，研究者应当理解各种方法的假设和局限性，根据具体问题选择合适的方法，并结合领域知识进行合理解释。