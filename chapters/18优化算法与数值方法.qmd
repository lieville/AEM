---
title: "18 数值优化与矩阵方法"
author: "李世纪"
format: html
editor: visual
---

## 本章导读

计量经济学的理论模型，无论是极大似然估计、广义矩估计，还是非线性最小二乘法，最终都需要通过数值计算转化为具体的参数估计值。本章深入探讨这一转化过程所依赖的**两大计算支柱**：**数值线性代数**与**数值优化算法**。前者为高效、稳定地处理数据与模型提供了基础数学工具，后者则利用这些工具，通过系统化的搜索策略求解最优化问题。

我们将揭示：矩阵分解如何成为构建稳健计算流程的"基石"，而优化算法如何作为使用这些基石构造解决方案的"建筑蓝图"。理解这两层架构，将使研究者从被动的软件使用者转变为能够洞察计算本质、诊断数值问题并针对特定问题选择适当方法的实证分析专家。这种能力对于应对高维数据、复杂模型和大规模计算等现代计量经济学的挑战至关重要。

**本章学习目标：** 1. 掌握核心矩阵分解的原理及其在计量计算中的应用场景 2. 理解主要优化算法的数学基础、收敛性质与适用条件 3. 学会诊断和处理常见的数值稳定性问题 4. 能够为特定计量问题设计合理的数值计算策略

## 17.1 引言：从理论估计量到数值实现

### 17.1.1 计量估计的计算本质

计量经济学中的大多数估计问题最终都可以归结为以下两类数值问题：

1.  **方程求解问题**：寻找参数向量 $\pmb{\theta} \in \mathbb{R}^p$ 使得一组矩条件成立： $$
    \pmb{g}_n(\pmb{\theta}) = \frac{1}{n}\sum_{i=1}^n \pmb{g}(\pmb{z}_i,\pmb{\theta}) = \pmb{0}
    $$ 其中 $\pmb{g}(\cdot)$ 是矩函数向量，$\pmb{z}_i$ 是第 $i$ 个观测值。

2.  **函数优化问题**：寻找参数 $\pmb{\theta}$ 最小化（或最大化）某个准则函数： $$
    \hat{\pmb{\theta}}_n = \arg\min_{\pmb{\theta} \in \Theta} Q_n(\pmb{\theta})
    $$ 其中 $Q_n(\pmb{\theta})$ 是样本准则函数。例如：

    -   在极大似然估计中，$Q_n(\pmb{\theta}) = -\frac{1}{n}\sum_{i=1}^n \ln f(\pmb{z}_i;\pmb{\theta})$
    -   在广义矩估计中，$Q_n(\pmb{\theta}) = \pmb{g}_n(\pmb{\theta})'\pmb{W}_n\pmb{g}_n(\pmb{\theta})$
    -   在非线性最小二乘中，$Q_n(\pmb{\theta}) = \frac{1}{n}\sum_{i=1}^n [y_i - h(\pmb{x}_i;\pmb{\theta})]^2$

这两种问题在本质上相互关联。一方面，优化问题的一阶条件通常是一个方程组；另一方面，许多方程求解问题可以通过构造适当的优化问题来更稳定地求解。

### 17.1.2 数值计算的三个核心关切

在实际实现计量估计时，我们需要同时关注三个相互关联又可能冲突的目标：

1.  **数值稳定性**：算法对数据扰动、舍入误差和病态问题的不敏感性。不稳定的算法可能在小样本或病态条件下给出荒谬的结果。

2.  **计算效率**：算法的时间和空间复杂度。随着数据维度 $p$ 和样本量 $n$ 的增长，计算成本可能成为瓶颈。

3.  **统计精度**：数值解与理论统计性质的吻合程度。即使是渐近无偏的估计量，也可能因数值误差而在有限样本中产生偏误。

这三者构成一个权衡三角（见图\@ref(fig:tradeoff-triangle)）。例如，奇异值分解（SVD）通常比Cholesky分解更稳定，但计算成本更高；牛顿法收敛速度快但可能数值不稳定；梯度下降法稳定但收敛缓慢。

### 17.1.3 本章的结构逻辑

本章按照从基础到应用、从通用到专用的逻辑展开：

1.  **基础工具层**（第17.2节）：介绍核心矩阵分解方法，这是所有高级计算的基础。
2.  **核心算法层**（第17.3-17.4节）：系统讲解主要优化算法的原理、性质和适用条件。
3.  **应用策略层**（第17.5节）：展示如何将基础工具与优化算法结合，解决实际计量问题。
4.  **前沿展望**（第17.6节）：探讨大规模计算、自动微分等现代发展。

## 17.2 数值线性代数基础：核心矩阵分解

矩阵分解是将复杂矩阵运算分解为简单、稳定、高效运算的数学技术。在计量计算中，它不仅是实现工具，更是理解数值稳定性的关键。

### 17.2.1 LU分解：通用线性系统求解器

**数学定义**：对于任意非奇异方阵 $A \in \mathbb{R}^{n \times n}$，LU分解将其表示为下三角矩阵 $L$ 和上三角矩阵 $U$ 的乘积： $$
A = LU
$$ 其中 $L$ 是单位下三角矩阵（对角线元素为1），$U$ 是上三角矩阵。

为了保证数值稳定性，实际中通常使用带行交换的LUP分解： $$
PA = LU
$$ 其中 $P$ 是置换矩阵。

**计量应用**： 1. **线性方程组求解**：求解 $A\pmb{x} = \pmb{b}$ 通过以下步骤： - 分解：$PA = LU$ - 求解：$L\pmb{y} = P\pmb{b}$（前向替代） - 求解：$U\pmb{x} = \pmb{y}$（后向替代）

2.  **行列式计算**：$\det(A) = \det(P^{-1})\det(U) = (-1)^s \prod_{i=1}^n u_{ii}$，其中 $s$ 是置换的符号。

3.  **矩阵求逆**：通过求解 $AX = I$ 获得。

**数值性质**： - 计算复杂度：$\frac{2}{3}n^3 + O(n^2)$ 次浮点运算 - 稳定性：部分主元法（LUP）通常足够稳定 - 局限性：要求矩阵非奇异，对病态矩阵敏感

### 17.2.2 Cholesky分解：对称正定系统的高效解法

**数学定义**：对于对称正定矩阵 $A$（即 $A = A'$ 且 $\pmb{x}'A\pmb{x} > 0$ 对所有 $\pmb{x} \neq \pmb{0}$），Cholesky分解表示为： $$
A = LL'
$$ 其中 $L$ 是下三角矩阵（对角线元素为正）。

**存在性与唯一性**：对称正定矩阵必有唯一的Cholesky分解，且 $L$ 的对角线元素 $l_{ii} > 0$。

**计量应用**： 1. **OLS估计的稳定计算**：求解正规方程 $(X'X)\pmb{\beta} = X'\pmb{y}$： - 计算 $X'X$ 的Cholesky分解：$X'X = LL'$ - 求解 $L\pmb{z} = X'\pmb{y}$（前向替代） - 求解 $L'\pmb{\beta} = \pmb{z}$（后向替代）

相比直接求逆 $(X'X)^{-1}$，Cholesky方法避免了显式计算逆矩阵，数值稳定性更好。

2.  **多元正态分布的模拟**：若 $\pmb{z} \sim N(\pmb{0}, I_n)$，则 $\pmb{x} = \pmb{\mu} + L\pmb{z} \sim N(\pmb{\mu}, LL' = A)$。

3.  **似然计算**：多元正态对数似然中的二次型和行列式： $$
    (\pmb{y}-\pmb{\mu})'A^{-1}(\pmb{y}-\pmb{\mu}) = \|L^{-1}(\pmb{y}-\pmb{\mu})\|^2
    $$ $$
    \ln|A| = 2\sum_{i=1}^n \ln l_{ii}
    $$ 通过Cholesky分解可稳定计算。

**数值性质**： - 计算复杂度：$\frac{1}{3}n^3 + O(n^2)$ 次浮点运算，约为LU分解的一半 - 稳定性：对称正定条件下非常稳定 - 病态诊断：当 $A$ 接近奇异时，$l_{ii}$ 会变得很小

### 17.2.3 QR分解：最小二乘问题的黄金标准

**数学定义**：对于任意矩阵 $A \in \mathbb{R}^{m \times n}$（$m \geq n$），QR分解为： $$
A = QR
$$ 其中 $Q \in \mathbb{R}^{m \times m}$ 是正交矩阵（$Q'Q = QQ' = I_m$），$R \in \mathbb{R}^{m \times n}$ 是上三角矩阵。经济型QR分解为： $$
A = Q_1 R_1
$$ 其中 $Q_1 \in \mathbb{R}^{m \times n}$ 列正交（$Q_1'Q_1 = I_n$），$R_1 \in \mathbb{R}^{n \times n}$ 上三角。

**计量应用**： 1. **线性回归的最小二乘解**：考虑问题 $\min_{\pmb{\beta}} \|\pmb{y} - X\pmb{\beta}\|^2$： - 计算 $X$ 的QR分解：$X = QR$ - 解 $\pmb{\beta} = R^{-1}Q'\pmb{y}$（实际通过回代求解）

关键优势：避免显式计算 $X'X$，从而避免因条件数平方而放大的数值误差。

2.  **回归诊断**：帽子矩阵 $H = X(X'X)^{-1}X' = QQ'$，其对角线元素（杠杆值）可直接从 $Q$ 获得。

3.  **秩亏回归**：当 $X$ 不满秩时，QR分解可通过列旋转揭示秩缺陷。

**稳定性分析**： QR分解的数值稳定性源于正交变换的范数保持性质。对于最小二乘问题，解 $\hat{\pmb{\beta}}$ 的相对误差满足： $$
\frac{\|\Delta\hat{\pmb{\beta}}\|}{\|\hat{\pmb{\beta}}\|} \leq \kappa(X)\left(\frac{\|\Delta X\|}{\|X\|} + \frac{\|\Delta\pmb{y}\|}{\|\pmb{y}\|}\right) + O(\epsilon^2)
$$ 其中 $\kappa(X) = \|X\|\|X^+\|$ 是条件数，$X^+$ 是伪逆。这比基于正规方程的方法（条件数为 $\kappa(X)^2$）有显著改进。

### 17.2.4 奇异值分解：诊断与稳健计算的终极工具

**数学定义**：对于任意矩阵 $A \in \mathbb{R}^{m \times n}$，SVD分解为： $$
A = U\Sigma V'
$$ 其中： - $U \in \mathbb{R}^{m \times m}$，$U'U = UU' = I_m$ - $V \in \mathbb{R}^{n \times n}$，$V'V = VV' = I_n$ - $\Sigma \in \mathbb{R}^{m \times n}$，对角矩阵，对角线元素 $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r \geq 0$ 为奇异值，$r = \text{rank}(A)$

**计量应用**： 1. **条件数诊断**：矩阵 $A$ 的2-范数条件数定义为： $$
   \kappa_2(A) = \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)} = \frac{\sigma_1}{\sigma_r}
   $$ 当 $\kappa_2(A)$ 很大时（如 $>10^3$），问题病态，OLS估计可能不可靠。

2.  **主成分回归**：对于病态设计矩阵 $X$，可构造： $$
    \hat{\pmb{\beta}}_{\text{PCR}} = \sum_{i=1}^k \frac{\pmb{u}_i'\pmb{y}}{\sigma_i}\pmb{v}_i, \quad k < r
    $$ 其中截断小奇异值相当于施加平滑约束。

3.  **广义逆计算**：Moore-Penrose伪逆为： $$
    A^+ = V\Sigma^+ U' = \sum_{i=1}^r \frac{1}{\sigma_i}\pmb{v}_i\pmb{u}_i'
    $$

4.  **降维技术**：主成分分析本质上是协方差矩阵的SVD。

**数值性质**： - 计算成本：$O(mn^2)$ 对 $m \geq n$，比QR分解昂贵 - 稳定性：非常稳定，可可靠计算秩和零空间 - 截断误差：秩 $k$ 近似 $A_k = \sum_{i=1}^k \sigma_i\pmb{u}_i\pmb{v}_i'$ 满足： $$
  \|A - A_k\|_2 = \sigma_{k+1}, \quad \|A - A_k\|_F = \sqrt{\sum_{i=k+1}^r \sigma_i^2}
  $$

### 17.2.5 矩阵分解方法的选择策略

选择适当的矩阵分解需要综合考虑问题结构、数值要求和计算约束。图\@ref(fig:decomp-decision)展示了基于问题特性的决策流程。

**关键决策因素**： 1. **矩阵结构**：是否对称？是否正定？ 2. **问题类型**：线性方程组？最小二乘？特征值问题？ 3. **数值要求**：是否需要最大稳定性？是否需要秩信息？ 4. **计算资源**：矩阵规模？可用内存？时间限制？

**实用指南**： - 对于对称正定线性系统：**Cholesky分解**（高效稳定） - 对于一般线性最小二乘：**QR分解**（稳定性与效率的平衡） - 对于病态或秩亏问题：**SVD分解**（最大稳定性，完整诊断） - 对于大规模稀疏问题：考虑稀疏矩阵格式和专门分解

## 17.3 无约束优化算法：寻找函数的极值

### 17.3.1 优化问题的数学框架与最优性条件

考虑无约束优化问题： $$
\min_{\pmb{\theta} \in \mathbb{R}^p} f(\pmb{\theta})
$$ 其中 $f: \mathbb{R}^p \rightarrow \mathbb{R}$ 是二阶连续可微的目标函数。

**一阶必要条件**（驻点条件）：若 $\pmb{\theta}^*$ 是局部极小点，则 $$
\nabla f(\pmb{\theta}^*) = \pmb{0}
$$ 其中 $\nabla f(\pmb{\theta}) = \left(\frac{\partial f}{\partial \theta_1}, \ldots, \frac{\partial f}{\partial \theta_p}\right)'$ 是梯度向量。

**二阶充分条件**：若 $\pmb{\theta}^*$ 满足： 1. $\nabla f(\pmb{\theta}^*) = \pmb{0}$ 2. $\nabla^2 f(\pmb{\theta}^*) \succ 0$（Hessian矩阵正定）

则 $\pmb{\theta}^*$ 是严格局部极小点。

**收敛速度的度量**： 设迭代序列 $\{\pmb{\theta}_k\}$ 收敛到 $\pmb{\theta}^*$，定义收敛速率： - 线性收敛：$\|\pmb{\theta}_{k+1} - \pmb{\theta}^*\| \leq c\|\pmb{\theta}_k - \pmb{\theta}^*\|$，$0 < c < 1$ - 超线性收敛：$\lim_{k\to\infty} \frac{\|\pmb{\theta}_{k+1} - \pmb{\theta}^*\|}{\|\pmb{\theta}_k - \pmb{\theta}^*\|} = 0$ - 二次收敛：$\|\pmb{\theta}_{k+1} - \pmb{\theta}^*\| \leq M\|\pmb{\theta}_k - \pmb{\theta}^*\|^2$，$M > 0$

### 17.3.2 迭代优化算法的通用架构

大多数迭代优化算法遵循以下模板：

1.  **初始化**：选择初始点 $\pmb{\theta}_0$，设定收敛容差 $\epsilon > 0$
2.  **迭代循环**（$k=0,1,2,\ldots$）：
    a.  **方向计算**：确定搜索方向 $\pmb{p}_k$
    b.  **步长选择**：确定步长 $\alpha_k > 0$
    c.  **参数更新**：$\pmb{\theta}_{k+1} = \pmb{\theta}_k + \alpha_k\pmb{p}_k$
    d.  **收敛检验**：若 $\|\nabla f(\pmb{\theta}_{k+1})\| < \epsilon$ 或满足其他停止准则，则终止
3.  **输出**：返回近似解 $\pmb{\theta}_{k+1}$

不同算法的区别主要在于方向 $\pmb{p}_k$ 的计算方式。

### 17.3.3 梯度下降法：基础但重要的基准方法

**算法原理**：梯度下降法使用目标函数的负梯度作为搜索方向： $$
\pmb{p}_k = -\nabla f(\pmb{\theta}_k)
$$ 更新公式为： $$
\pmb{\theta}_{k+1} = \pmb{\theta}_k - \alpha_k \nabla f(\pmb{\theta}_k)
$$

**步长选择策略**： 1. **固定步长**：$\alpha_k \equiv \alpha$，简单但不适应曲率变化 2. **精确线搜索**：$\alpha_k = \arg\min_{\alpha>0} f(\pmb{\theta}_k + \alpha\pmb{p}_k)$，计算成本高 3. **回溯线搜索**（Armijo准则）：选择 $\alpha_k$ 使得： $$
   f(\pmb{\theta}_k + \alpha_k\pmb{p}_k) \leq f(\pmb{\theta}_k) + c\alpha_k\nabla f(\pmb{\theta}_k)'\pmb{p}_k
   $$ 其中 $c \in (0,1)$，通常 $c=10^{-4}$

**收敛性质**： - 对于强凸且L-光滑函数（$\mu I \preceq \nabla^2 f(\pmb{\theta}) \preceq LI$），梯度下降法线性收敛： $$
  f(\pmb{\theta}_k) - f(\pmb{\theta}^*) \leq \left(1 - \frac{\mu}{L}\right)^k [f(\pmb{\theta}_0) - f(\pmb{\theta}^*)]
  $$ - 收敛速率取决于条件数 $\kappa = L/\mu$，$\kappa$ 越大收敛越慢 - 实际应用中常因条件数大而表现不佳

**在计量经济学中的角色**： 虽然梯度下降法很少作为最终求解器，但它作为： 1. **基准方法**：用于对比更复杂算法的性能 2. **预处理步骤**：在更精细算法前进行粗略优化 3. **随机变体**：随机梯度下降是大规模机器学习的基础

### 17.3.4 牛顿法：利用曲率信息的快速方法

**算法原理**：牛顿法基于目标函数的二阶泰勒展开： $$
f(\pmb{\theta}_k + \pmb{p}) \approx f(\pmb{\theta}_k) + \nabla f(\pmb{\theta}_k)'\pmb{p} + \frac{1}{2}\pmb{p}'\nabla^2 f(\pmb{\theta}_k)\pmb{p}
$$ 最小化该二次近似得到牛顿方向： $$
\pmb{p}_k^{\text{Newton}} = -[\nabla^2 f(\pmb{\theta}_k)]^{-1}\nabla f(\pmb{\theta}_k)
$$

**算法特性**： 1. **收敛速度**：在解附近，若 $\nabla^2 f(\pmb{\theta}^*)$ 正定且Lipschitz连续，则牛顿法二次收敛： $$
   \|\pmb{\theta}_{k+1} - \pmb{\theta}^*\| \leq M\|\pmb{\theta}_k - \pmb{\theta}^*\|^2
   $$

2.  **不变性**：牛顿法在参数仿射变换下不变，而梯度下降法不变。

3.  **计算需求**：每步需要计算Hessian矩阵 $\nabla^2 f(\pmb{\theta}_k)$ 并求解线性系统，复杂度 $O(p^3)$。

**数值实现的关键问题**： 1. **Hessian正定性**：牛顿方向是下降方向当且仅当 $\nabla^2 f(\pmb{\theta}_k)$ 正定。在非凸区域可能不成立。 2. **线性系统求解**：需要稳定求解 $\nabla^2 f(\pmb{\theta}_k)\pmb{p} = -\nabla f(\pmb{\theta}_k)$，通常使用： - Cholesky分解（如果Hessian正定） - LU分解（一般情况） - QR/SVD分解（病态情况） 3. **步长控制**：纯牛顿步（$\alpha_k=1$）可能不下降，需结合线搜索。

**修正牛顿法**： 为保证下降方向和数值稳定性，常用修正策略： 1. **阻尼牛顿法**：$\pmb{p}_k = -(\nabla^2 f(\pmb{\theta}_k) + \mu_k I)^{-1}\nabla f(\pmb{\theta}_k)$，$\mu_k \geq 0$ 2. **修改Cholesky分解**：将Hessian分解为 $LDL'$ 并确保 $D$ 的对角元足够正

**在计量经济学中的应用**： 牛顿法是极大似然估计的**标准算法**，因为： 1. MLE的渐近性质保证Hessian在解附近正定（等于信息矩阵的负值） 2. 二次收敛意味着很少迭代即可达到高精度 3. 计算Hessian的额外成本常被快速收敛所抵消

### 17.3.5 拟牛顿法：平衡效率与稳定性的计量主力

**核心思想**：拟牛顿法构造Hessian矩阵的近似 $B_k \approx \nabla^2 f(\pmb{\theta}_k)$ 或其逆 $H_k \approx [\nabla^2 f(\pmb{\theta}_k)]^{-1}$，仅使用一阶信息（梯度）更新。

**拟牛顿条件**（割线方程）： $$
B_{k+1}(\pmb{\theta}_{k+1} - \pmb{\theta}_k) = \nabla f(\pmb{\theta}_{k+1}) - \nabla f(\pmb{\theta}_k)
$$ 记 $\pmb{s}_k = \pmb{\theta}_{k+1} - \pmb{\theta}_k$，$\pmb{y}_k = \nabla f(\pmb{\theta}_{k+1}) - \nabla f(\pmb{\theta}_k)$，则条件为： $$
B_{k+1}\pmb{s}_k = \pmb{y}_k
$$

**BFGS公式**（Broyden-Fletcher-Goldfarb-Shanno）： 这是最成功的拟牛顿更新之一。逆Hessian近似 $H_k = B_k^{-1}$ 的BFGS更新为： $$
H_{k+1} = \left(I - \frac{\pmb{s}_k\pmb{y}_k'}{\pmb{y}_k'\pmb{s}_k}\right)H_k\left(I - \frac{\pmb{y}_k\pmb{s}_k'}{\pmb{y}_k'\pmb{s}_k}\right) + \frac{\pmb{s}_k\pmb{s}_k'}{\pmb{y}_k'\pmb{s}_k}
$$

**BFGS的性质**： 1. **正定性保持**：若 $H_k \succ 0$ 且 $\pmb{y}_k'\pmb{s}_k > 0$（曲率条件），则 $H_{k+1} \succ 0$ 2. **超线性收敛**：在适当条件下，BFGS超线性收敛 3. **自我校正**：BFGS更新能自动纠正近似误差 4. **计算效率**：每步 $O(p^2)$ 操作，无需计算或存储Hessian

**L-BFGS**（有限内存BFGS）： 对于大规模问题（$p$ 很大），存储 $p \times p$ 矩阵 $H_k$ 不可行。L-BFGS只保存最近的 $m$ 组 $(\pmb{s}_i, \pmb{y}_i)$ 对（通常 $m=5\sim20$），通过递归公式计算矩阵-向量乘积 $H_k\nabla f(\pmb{\theta}_k)$。

**双循环递归算法**： L-BFGS通过以下两步计算搜索方向 $\pmb{p}_k = -H_k\nabla f(\pmb{\theta}_k)$： 1. **前向循环**：利用最近的历史信息 2. **后向循环**：对称地应用更新

复杂度为 $O(mp)$，内存需求 $O(mp)$。

**SR1公式**（对称秩1更新）： 另一种重要的拟牛顿更新： $$
B_{k+1} = B_k + \frac{(\pmb{y}_k - B_k\pmb{s}_k)(\pmb{y}_k - B_k\pmb{s}_k)'}{(\pmb{y}_k - B_k\pmb{s}_k)'\pmb{s}_k}
$$ SR1不强制正定性，但有时能产生更好的Hessian近似，特别在非凸问题中。

**拟牛顿法的计量应用**： 在计量经济学中，拟牛顿法尤其是BFGS，通常是**极大似然估计的首选算法**，因为： 1. 避免了Hessian的计算，对复杂模型特别有利 2. 保持了牛顿法的快速收敛特性 3. 对线搜索不敏感，实现相对简单 4. 软件包（如Stata的`ml`、R的`optim`）常默认使用BFGS或其变体

## 17.4 稳健与专用优化策略

### 17.4.1 信任域法：鲁棒的牛顿类方法

**基本思想**：信任域法在每次迭代中，在当前点 $\pmb{\theta}_k$ 周围定义一个信任域 $\|\pmb{d}\| \leq \Delta_k$，在这个区域内优化局部模型。相比线搜索方法（先定方向再找步长），信任域法同时确定方向和步长。

**数学表述**：在第 $k$ 步，求解子问题： $$
\min_{\pmb{d} \in \mathbb{R}^p} m_k(\pmb{d}) = f(\pmb{\theta}_k) + \nabla f(\pmb{\theta}_k)'\pmb{d} + \frac{1}{2}\pmb{d}'B_k\pmb{d}
$$ $$
\text{s.t.} \quad \|\pmb{d}\| \leq \Delta_k
$$ 其中 $B_k$ 是Hessian或其近似，$\|\cdot\|$ 通常是欧几里得范数或其对等范数。

**算法框架**： 1. **模型选择**：构造局部二次模型 $m_k(\pmb{d})$ 2. **子问题求解**：在信任域内最小化 $m_k(\pmb{d})$ 3. **接受性检验**：计算实际下降与预测下降的比率： $$
   \rho_k = \frac{f(\pmb{\theta}_k) - f(\pmb{\theta}_k + \pmb{d}_k)}{m_k(\pmb{0}) - m_k(\pmb{d}_k)}
   $$ 4. **信任域调整**： - 若 $\rho_k$ 接近1，接受步长，可能扩大 $\Delta_k$ - 若 $\rho_k$ 很小，拒绝步长，缩小 $\Delta_k$ - 若 $\rho_k$ 中等，接受步长，保持 $\Delta_k$

**子问题求解方法**： 1. **柯西点法**：沿最速下降方向到信任域边界 2. **狗腿法**：沿最速下降方向与牛顿方向的折衷路径 3. **截断共轭梯度法**：在边界内应用共轭梯度法 4. **精确解**：通过求解 $\nabla m_k(\pmb{d}) = -\lambda\pmb{d}$，$\lambda \geq 0$

**优势与应用场景**： 1. **鲁棒性强**：对初始点和非凸区域不敏感 2. **处理不定Hessian**：信任域约束自然处理不定二次模型 3. **适用于非线性最小二乘**：在Gauss-Newton和Levenberg-Marquardt算法中特别有效 4. **计量应用**：常用于复杂的结构方程模型、非凸似然函数

**Levenberg-Marquardt算法**：非线性最小二乘的信任域特例，其中： $$
B_k = J_k'J_k + \lambda_k I
$$ $J_k$ 是残差函数的雅可比矩阵，$\lambda_k$ 控制信任域大小。

### 17.4.2 Nelder-Mead单纯形法：无导数优化

**算法思想**：Nelder-Mead法（下坡单纯形法）通过比较单纯形顶点的函数值，进行反射、扩张、收缩等几何操作，适用于导数不可用、不可靠或计算昂贵的情况。

**算法步骤**： 设单纯形有 $p+1$ 个顶点 $\pmb{\theta}_0, \pmb{\theta}_1, \ldots, \pmb{\theta}_p$，对应函数值 $f_0 \leq f_1 \leq \cdots \leq f_p$。

1.  **排序**：$f(\pmb{\theta}_0) \leq f(\pmb{\theta}_1) \leq \cdots \leq f(\pmb{\theta}_p)$
2.  **计算重心**：$\bar{\pmb{\theta}} = \frac{1}{p}\sum_{i=0}^{p-1} \pmb{\theta}_i$（排除最差点 $\pmb{\theta}_p$）
3.  **反射**：$\pmb{\theta}_r = \bar{\pmb{\theta}} + \alpha(\bar{\pmb{\theta}} - \pmb{\theta}_p)$，$\alpha > 0$（通常 $\alpha=1$）
4.  **决策**：
    -   若 $f_0 \leq f_r < f_{p-1}$：用 $\pmb{\theta}_r$ 替换 $\pmb{\theta}_p$
    -   若 $f_r < f_0$：**扩张**：$\pmb{\theta}_e = \bar{\pmb{\theta}} + \gamma(\pmb{\theta}_r - \bar{\pmb{\theta}})$，$\gamma > 1$（通常 $\gamma=2$）
        -   若 $f_e < f_r$：接受 $\pmb{\theta}_e$
        -   否则：接受 $\pmb{\theta}_r$
    -   若 $f_r \geq f_{p-1}$：**收缩**：
        -   若 $f_r < f_p$：外收缩 $\pmb{\theta}_c = \bar{\pmb{\theta}} + \beta(\pmb{\theta}_r - \bar{\pmb{\theta}})$，$\beta \in (0,1)$（通常 $\beta=0.5$）
        -   若 $f_r \geq f_p$：内收缩 $\pmb{\theta}_c = \bar{\pmb{\theta}} + \beta(\pmb{\theta}_p - \bar{\pmb{\theta}})$
        -   若 $f_c < \min(f_r, f_p)$：接受 $\pmb{\theta}_c$
        -   否则：**缩小**单纯形，向最好点 $\pmb{\theta}_0$ 收缩

**算法特性**： 1. **无需求导**：只依赖函数值比较 2. **适应性强**：能处理不连续、不可微函数 3. **收敛性**：理论上可能不收敛到驻点，实践中常有效 4. **维度限制**：通常适用于 $p \leq 10$ 的中小规模问题

**计量应用**： 1. **初始值生成**：为梯度基方法提供好的起点 2. **非标准模型**：目标函数不可微或导数难以计算时 3. **模型调试**：快速获得参数的大致范围 4. **鲁棒估计**：某些稳健估计量（如LAD）的求解

### 17.4.3 EM算法：潜变量与缺失数据问题的专用框架

**问题背景**：当观测数据 $\pmb{y}$ 不完整，存在缺失数据或潜变量 $\pmb{z}$ 时，直接最大化观测数据似然 $f(\pmb{y};\pmb{\theta})$ 可能困难。EM算法通过引入完整数据 $(\pmb{y},\pmb{z})$ 简化问题。

**算法框架**： 给定当前估计 $\pmb{\theta}^{(t)}$，EM算法迭代： 1. **E步（期望步）**：计算Q函数： $$
   Q(\pmb{\theta}|\pmb{\theta}^{(t)}) = \mathbb{E}_{\pmb{z}|\pmb{y},\pmb{\theta}^{(t)}}[\ln f(\pmb{y},\pmb{z};\pmb{\theta})]
   $$ 2. **M步（最大化步）**：更新参数： $$
   \pmb{\theta}^{(t+1)} = \arg\max_{\pmb{\theta}} Q(\pmb{\theta}|\pmb{\theta}^{(t)})
   $$

**收敛性质**： 1. **单调性**：观测数据似然不减：$\ell(\pmb{\theta}^{(t+1)};\pmb{y}) \geq \ell(\pmb{\theta}^{(t)};\pmb{y})$ 2. **收敛到驻点**：在适当条件下，$\pmb{\theta}^{(t)}$ 收敛到似然函数的驻点 3. **收敛速度**：线性收敛，速度依赖于信息缺失比例

**EM作为优化算法**： 可将EM视为一种特殊的优化算法，其中： - 方向：由Q函数与当前似然的梯度差决定 - 步长：隐式由E步和M步确定

**加速变体**： 1. **ECM**（期望条件最大化）：将M步分解为多个条件最大化，简化计算 2. **ECME**（期望条件最大化要么）：某些条件最大化直接针对观测似然而非Q函数 3. **PX-EM**（参数扩展EM）：引入辅助参数加速收敛

**计量应用**： 1. **混合模型**：有限混合分布、隐马尔可夫模型 2. **面板数据**：带有个体效应的非线性面板模型 3. **生存分析**：包含删失数据的模型 4. **因子分析**：潜变量结构方程模型

### 17.4.4 坐标下降法与近端梯度法：高维稀疏模型求解

**坐标下降法原理**：每次迭代只优化一个坐标（变量），固定其他坐标： $$
\theta_j^{(k+1)} = \arg\min_{\theta_j} f(\theta_1^{(k+1)}, \ldots, \theta_{j-1}^{(k+1)}, \theta_j, \theta_{j+1}^{(k)}, \ldots, \theta_p^{(k)})
$$ 循环或随机遍历所有坐标。

**收敛条件**： 1. 若 $f$ 凸且可微，且每个坐标最小化有唯一解，则收敛到全局最优 2. 对于非凸问题，收敛到驻点

**计算优势**： 1. **子问题简单**：单变量优化可能有解析解 2. **内存效率**：每步只更新一个变量 3. **并行潜力**：某些变体可并行计算

**LASSO问题的坐标下降**： 考虑LASSO问题： $$
\min_{\pmb{\beta}} \frac{1}{2n}\|\pmb{y} - X\pmb{\beta}\|^2 + \lambda\|\pmb{\beta}\|_1
$$ 坐标更新公式为： $$
\beta_j^{\text{new}} = S\left(\frac{1}{n}\sum_{i=1}^n x_{ij}\left(y_i - \sum_{k\neq j} x_{ik}\beta_k\right), \lambda\right)
$$ 其中 $S(z,\lambda) = \text{sign}(z)(|z| - \lambda)_+$ 是软阈值函数。

**近端梯度法**：适用于复合优化问题： $$
\min_{\pmb{\theta}} f(\pmb{\theta}) = g(\pmb{\theta}) + h(\pmb{\theta})
$$ 其中 $g$ 可微，$h$ 可能不可微但"简单"（近端算子易计算）。

**迭代格式**： $$
\pmb{\theta}^{(k+1)} = \text{prox}_{\alpha h}\left(\pmb{\theta}^{(k)} - \alpha\nabla g(\pmb{\theta}^{(k)})\right)
$$ 其中近端算子定义为： $$
\text{prox}_{\alpha h}(\pmb{v}) = \arg\min_{\pmb{\theta}} \left\{h(\pmb{\theta}) + \frac{1}{2\alpha}\|\pmb{\theta} - \pmb{v}\|^2\right\}
$$

**FISTA**（快速迭代收缩阈值算法）： Nesterov加速的近端梯度法，用于LASSO等问题，达到最优收敛速率 $O(1/k^2)$。

**计量应用**： 1. **高维回归**：LASSO、弹性网、稀疏组LASSO 2. **结构方程模型**：带稀疏约束的协方差矩阵估计 3. **时间序列**：向量自回归的稀疏估计 4. **图形模型**：高斯图模型的结构学习

## 17.5 综合应用：计量估计的数值实现策略

### 17.5.1 极大似然估计的完整数值流程

以Logit模型为例，展示MLE的系统化实现策略。

**模型设定**： 二值选择模型 $y_i \in \{0,1\}$，条件概率： $$
P(y_i=1|\pmb{x}_i) = \Lambda(\pmb{x}_i'\pmb{\beta}) = \frac{\exp(\pmb{x}_i'\pmb{\beta})}{1 + \exp(\pmb{x}_i'\pmb{\beta})}
$$ 对数似然函数： $$
\ell(\pmb{\beta}) = \sum_{i=1}^n \left[y_i \ln \Lambda(\pmb{x}_i'\pmb{\beta}) + (1-y_i)\ln(1-\Lambda(\pmb{x}_i'\pmb{\beta}))\right]
$$

**梯度与Hessian**： 记 $p_i = \Lambda(\pmb{x}_i'\pmb{\beta})$，则有： $$
\nabla\ell(\pmb{\beta}) = \sum_{i=1}^n (y_i - p_i)\pmb{x}_i = X'(\pmb{y} - \pmb{p})
$$ $$
\nabla^2\ell(\pmb{\beta}) = -\sum_{i=1}^n p_i(1-p_i)\pmb{x}_i\pmb{x}_i' = -X'DX
$$ 其中 $D = \text{diag}\{p_i(1-p_i)\}$。

**数值实现策略**：

1.  **初始值选择**：

    -   使用线性概率模型：$\pmb{\beta}^{(0)} = (X'X)^{-1}X'\pmb{y}$
    -   或零向量：$\pmb{\beta}^{(0)} = \pmb{0}$

2.  **优化算法选择**：

    -   **牛顿法**：需要计算和求逆Hessian，由于 $-X'DX$ 半负定，牛顿方向： $$
        \pmb{\beta}^{(k+1)} = \pmb{\beta}^{(k)} + (X'D^{(k)}X)^{-1}X'(\pmb{y} - \pmb{p}^{(k)})
        $$ 这是**迭代加权最小二乘**（IRLS）形式。
    -   **拟牛顿法（BFGS）**：避免构造和求逆Hessian，内存效率高。
    -   **信任域法**：当 $X'DX$ 接近奇异时更稳定。

3.  **收敛准则**：

    -   梯度范数：$\|\nabla\ell(\pmb{\beta}^{(k)})\| < \epsilon_1$
    -   参数变化：$\|\pmb{\beta}^{(k+1)} - \pmb{\beta}^{(k)}\| < \epsilon_2(1+\|\pmb{\beta}^{(k)}\|)$
    -   函数值变化：$|\ell(\pmb{\beta}^{(k+1)}) - \ell(\pmb{\beta}^{(k)})| < \epsilon_3(1+|\ell(\pmb{\beta}^{(k)})|)$

4.  **数值稳定性措施**：

    -   **概率裁剪**：计算 $p_i$ 时避免数值溢出，如 $p_i = \max(\epsilon, \min(1-\epsilon, \Lambda(\pmb{x}_i'\pmb{\beta})))$，$\epsilon=10^{-8}$
    -   **正则化**：在Hessian中添加小扰动，$(X'DX + \delta I)^{-1}$，$\delta=10^{-6}$
    -   **重新参数化**：对高度相关的协变量进行正交化

5.  **标准误计算**： 信息矩阵估计：$\hat{I}(\hat{\pmb{\beta}}) = - \nabla^2\ell(\hat{\pmb{\beta}}) = X'\hat{D}X$ 协方差矩阵：$\widehat{\text{Var}}(\hat{\pmb{\beta}}) = [X'\hat{D}X]^{-1}$

    当 $X'\hat{D}X$ 病态时，使用：

    -   Cholesky分解加扰动
    -   QR分解
    -   SVD截断伪逆

### 17.5.2 病态问题的诊断与处理

**病态性来源**： 1. **近似多重共线性**：设计矩阵 $X$ 列近似线性相关 2. **尺度差异**：协变量量纲差异巨大 3. **分离或拟分离**：在Logit/Probit模型中，某些协变量组合完美预测结果 4. **稀疏数据**：某些协变量取值变化很小

**诊断工具**： 1. **条件数**：$\kappa(X) = \|X\|\|X^+\|$，$\kappa(X'X) = \kappa(X)^2$ - $\kappa < 10^2$：良态 - $10^2 \leq \kappa < 10^3$：轻度病态 - $\kappa \geq 10^3$：严重病态 2. **方差膨胀因子**：$\text{VIF}_j = 1/(1-R_j^2)$，$R_j^2$ 是 $x_j$ 对其他协变量的回归 $R^2$ - $\text{VIF} > 10$ 表明严重共线性 3. **奇异值分解**：小奇异值 $\sigma_r/\sigma_1 < 10^{-6}$ 表明数值秩亏 4. **相关性矩阵**：绝对值接近1的相关系数

**处理策略**： 1. **变量选择**：剔除高度相关的变量 2. **正则化**： - 岭回归：$\min_{\pmb{\beta}} \|\pmb{y} - X\pmb{\beta}\|^2 + \lambda\|\pmb{\beta}\|^2$ - LASSO：$\min_{\pmb{\beta}} \|\pmb{y} - X\pmb{\beta}\|^2 + \lambda\|\pmb{\beta}\|_1$ 3. **主成分回归**：用 $X$ 的主成分作为新设计矩阵 4. **重新参数化**： - 中心化：$x_{ij} \leftarrow x_{ij} - \bar{x}_j$ - 标准化：$x_{ij} \leftarrow (x_{ij} - \bar{x}_j)/s_j$ - 正交多项式：对于多项式项 5. **增加数据**：收集更多样本或设计实验打破共线性

**数值稳定算法选择**： 1. 对于线性回归：**QR分解**或**SVD**而非正规方程 2. 对于非线性最小二乘：**Levenberg-Marquardt**（带阻尼的Gauss-Newton） 3. 对于MLE：**信任域牛顿法**或带正则化的拟牛顿法 4. 对于高维问题：**坐标下降法**或**近端梯度法**

### 17.5.3 收敛失败的原因与调试策略

**常见收敛问题**： 1. **不收敛**：迭代在有限步内未达到收敛准则 2. **收敛到错误点**：局部最优而非全局最优 3. **收敛过慢**：需要过多迭代 4. **数值溢出**：函数值、梯度或Hessian中出现NaN或Inf

**诊断步骤**： 1. **检查梯度**：计算有限差分梯度与解析梯度比较： $$
   \frac{\|\nabla f_{\text{analytic}} - \nabla f_{\text{finite-diff}}\|}{\|\nabla f_{\text{analytic}}\| + 1} < 10^{-6}
   $$ 2. **检查Hessian**：验证正定性，计算最小特征值 3. **轨迹分析**：记录每次迭代的函数值、梯度范数、步长 4. **条件数检查**：计算Hessian或设计矩阵的条件数

**调试策略**： 1. **尝试不同初始值**：使用网格搜索、随机抽样或简化模型获得初始值 2. **调整算法参数**： - 线搜索参数（Wolfe条件常数） - 信任域半径初始值和更新策略 - 正则化参数 3. **变换参数空间**： - 对数变换：$\theta \leftarrow \exp(\phi)$ 对正参数 - Logit变换：$\theta \leftarrow \Lambda(\phi)$ 对 $(0,1)$ 内参数 - 标准化：使参数量级相近 4. **简化模型**：先估计简化形式，逐步增加复杂度 5. **使用鲁棒算法**：从单纯形法开始，然后切换到梯度基方法

**软件实现提示**： 1. **梯度检查**：大多数优化库提供梯度检查选项 2. **详细输出**：请求输出每次迭代的信息 3. **多种算法尝试**：比较不同算法的结果 4. **缩放选项**：利用软件的自动缩放功能

## 17.6 前沿发展与展望

### 17.6.1 大规模优化：随机方法与分布式计算

**随机梯度下降**：对于样本量 $n$ 很大的问题，计算全梯度 $\nabla f(\pmb{\theta}) = \frac{1}{n}\sum_{i=1}^n \nabla f_i(\pmb{\theta})$ 成本高。SGD每次迭代使用单个或小批量样本： $$
\pmb{\theta}_{k+1} = \pmb{\theta}_k - \alpha_k \nabla f_{i_k}(\pmb{\theta}_k)
$$

**自适应方法**： 1. **AdaGrad**：为每个参数调整学习率 2. **RMSProp**：使用指数加权移动平均调整 3. **Adam**：结合动量和自适应学习率

**分布式优化**： 1. **同步并行**：参数服务器架构，所有工作节点同步更新 2. **异步并行**：允许延迟更新，减少通信开销 3. **联邦学习**：分散数据下的隐私保护优化

### 17.6.2 自动微分：精确高效求导

**基本原理**：自动微分通过计算图追踪运算，应用链式法则，提供精确到机器精度的导数。

**两种模式**： 1. **前向模式**：计算 $\dot{y} = \nabla f(\pmb{x}) \cdot \dot{\pmb{x}}$，适合输入维度低的情况 2. **反向模式**：计算 $\bar{\pmb{x}} = \nabla f(\pmb{x})' \cdot \bar{y}$，适合输出维度低的情况（最常见）

**优势**： 1. 比有限差分更精确 2. 比符号微分更高效 3. 方便实现高阶导数 4. 与现代机器学习框架（TensorFlow, PyTorch）集成

**在计量经济学中的应用前景**： 1. 复杂结构模型的梯度计算 2. 基于梯度的贝叶斯计算（HMC, NUTS） 3. 高维模型的正则化路径计算

### 17.6.3 贝叶斯计算中的优化视角

**最大后验估计**：MAP估计可视为带先验的MLE： $$
\hat{\pmb{\theta}}_{\text{MAP}} = \arg\max_{\pmb{\theta}} [\ell(\pmb{\theta};\pmb{y}) + \ln p(\pmb{\theta})]
$$

**变分推断**：将后验分布 $p(\pmb{\theta}|\pmb{y})$ 近似为简单分布 $q(\pmb{\theta};\pmb{\phi})$，通过优化证据下界（ELBO）： $$
\text{ELBO}(\pmb{\phi}) = \mathbb{E}_{q}[\ln p(\pmb{y},\pmb{\theta}) - \ln q(\pmb{\theta};\pmb{\phi})]
$$

**随机变分推断**：结合随机梯度与自然梯度，处理大规模数据。

**优化与抽样的结合**： 1. **哈密顿蒙特卡洛**：使用梯度信息指导MCMC采样 2. **朗之万动力学**：带噪声的梯度下降，连接优化与抽样 3. **模拟退火**：从优化到抽样的温度调度

### 17.6.4 计算思维的培养

**从封闭形式到数值解**： 传统计量教学强调存在解析解的特殊情况，但现实问题多需数值解。计算思维包括： 1. 将理论估计量转化为可计算形式 2. 理解数值算法的假设与局限 3. 诊断和解决计算问题 4. 验证数值结果的可靠性

**可重复计算实践**： 1. **代码文档化**：记录算法选择、参数设置、收敛准则 2. **敏感性分析**：检查结果对初始值、算法参数、数值容差的敏感性 3. **基准测试**：与已知解或替代方法比较 4. **版本控制**：跟踪代码和数据的变化

**跨学科工具借鉴**： 1. 从数值分析借鉴稳定算法 2. 从优化理论借鉴收敛分析 3. 从计算机科学借鉴数据结构与算法 4. 从机器学习借鉴大规模优化方法

## 本章总结

本章系统构建了计量经济学数值实现的知识体系，涵盖了从基础矩阵分解到高级优化算法的完整链条。

**核心要点回顾**：

1.  **矩阵分解是数值稳定性的基石**：
    -   **Cholesky分解** 为对称正定系统提供高效求解
    -   **QR分解** 是线性最小二乘的黄金标准，避免条件数平方
    -   **SVD分解** 提供最完整的矩阵分析和稳健计算
    -   分解方法的选择应基于问题结构、数值需求和计算约束
2.  **优化算法是参数估计的引擎**：
    -   **梯度下降法** 是基础基准，适合大规模问题但收敛慢
    -   **牛顿法** 利用二阶信息快速收敛，是MLE的标准选择
    -   **拟牛顿法（BFGS）** 平衡效率与稳定性，是计量实践的主力
    -   **信任域法** 更鲁棒，适合非凸问题和不定Hessian
    -   **坐标下降法** 是高维稀疏模型的高效求解器
3.  **专用算法解决特定问题**：
    -   **EM算法** 处理缺失数据和潜变量
    -   **Nelder-Mead法** 在导数不可用时提供无导数优化
    -   **近端梯度法** 处理非光滑正则化项
4.  **系统化实现策略**：
    -   从合理初始值开始
    -   选择适合问题特性的算法
    -   实施数值稳定性措施
    -   建立全面的收敛诊断
    -   验证结果的可靠性

**关键启示**：

计量经济学的数值实现不是简单的"黑箱"操作，而是需要深入理解的科学过程。成功的数值实现需要：

1.  **算法与问题的匹配**：没有一种算法适合所有问题。理解算法的假设、收敛性质和数值行为是选择合适算法的前提。
2.  **稳定性优先于速度**：在计量应用中，获得稳定、可靠的结果比快速计算更重要。有时需要牺牲一些效率来保证数值稳定性。
3.  **诊断驱动的开发**：实施系统化的诊断流程，包括梯度检查、条件数分析、收敛轨迹监控等。
4.  **分层设计**：从简单模型开始，逐步增加复杂性；从鲁棒算法开始，再切换到高效算法。

**未来方向**：

随着计量经济学问题日益复杂和数据规模不断增长，数值计算方法的重要性只会增加。值得关注的发展包括：

1.  **自动化算法选择**：基于问题特征自动推荐合适算法
2.  **混合方法**：结合不同算法的优势，如随机方法与二阶方法结合
3.  **硬件感知计算**：利用GPU、TPU等专用硬件加速
4.  **不确定性量化**：不仅提供点估计，还量化数值误差的影响

掌握本章介绍的工具和思维，将使研究者能够更自信地处理复杂的计量模型，更深入地理解软件输出背后的计算过程，并在面对新的计量挑战时设计有效的数值解决方案。

## 本章习题

### 理论习题

1.  **矩阵分解比较**：设 $X \in \mathbb{R}^{n \times p}$ 是列满秩设计矩阵，$n > p$。比较求解OLS估计 $\hat{\pmb{\beta}} = (X'X)^{-1}X'\pmb{y}$ 的三种方法：直接求逆、Cholesky分解和QR分解。
    -   推导每种方法的计算复杂度（以浮点运算次数表示）
    -   分析每种方法的数值稳定性，特别是当 $X$ 病态时
    -   说明在什么条件下应选择哪种方法
2.  **收敛性分析**：考虑梯度下降法应用于强凸且L-光滑函数 $f: \mathbb{R}^p \rightarrow \mathbb{R}$，即存在 $\mu, L > 0$ 使得 $\mu I \preceq \nabla^2 f(\pmb{\theta}) \preceq LI$ 对所有 $\pmb{\theta}$ 成立。
    -   证明固定步长 $\alpha = 1/L$ 的梯度下降法满足： $$
        f(\pmb{\theta}_k) - f(\pmb{\theta}^*) \leq \left(1 - \frac{\mu}{L}\right)^k [f(\pmb{\theta}_0) - f(\pmb{\theta}^*)]
        $$
    -   解释条件数 $\kappa = L/\mu$ 如何影响收敛速度
    -   对比梯度下降法与牛顿法在相同假设下的收敛速率
3.  **拟牛顿条件与更新唯一性**：设 $B_k$ 是Hessian近似，$\pmb{s}_k = \pmb{\theta}_{k+1} - \pmb{\theta}_k$，$\pmb{y}_k = \nabla f(\pmb{\theta}_{k+1}) - \nabla f(\pmb{\theta}_k)$。
    -   证明满足拟牛顿条件 $B_{k+1}\pmb{s}_k = \pmb{y}_k$ 且 $B_{k+1} - B_k$ 秩最小的更新是SR1（对称秩1）更新
    -   证明在 $B_{k+1} - B_k$ 秩为2且对称正定的约束下，BFGS更新是唯一的
    -   讨论为何BFGS在实践中比SR1更常用
4.  **信任域法的全局收敛**：考虑信任域法应用于一般非线性函数 $f(\pmb{\theta})$，局部模型为 $m_k(\pmb{d}) = f(\pmb{\theta}_k) + \nabla f(\pmb{\theta}_k)'\pmb{d} + \frac{1}{2}\pmb{d}'B_k\pmb{d}$，其中 $B_k$ 对称。
    -   定义柯西点 $\pmb{d}_k^c$ 并证明它至少提供与最速下降方向成比例的下降量
    -   证明如果每次迭代选择的步长 $\pmb{d}_k$ 至少提供与柯西点成比例的下降，且 $B_k$ 一致有界，则算法全局收敛到驻点
    -   对比信任域法与线搜索方法在全局收敛性保证方面的差异

### 应用习题

5.  **Logit模型MLE的实现设计**：考虑二值Logit模型 $P(y_i=1|\pmb{x}_i) = \Lambda(\pmb{x}_i'\pmb{\beta})$，样本量为 $n$，协变量维度为 $p$。
    -   设计基于牛顿法的完整实现方案，包括初始值选择、迭代格式、收敛准则和标准误计算
    -   讨论当 $X'DX$ 接近奇异时的处理策略，其中 $D = \text{diag}\{\Lambda(\pmb{x}_i'\pmb{\beta})[1-\Lambda(\pmb{x}_i'\pmb{\beta})]\}$
    -   对比牛顿法与拟牛顿法（BFGS）在此问题上的计算复杂度和存储需求
6.  **病态回归问题的诊断与处理**：假设在线性回归 $y = X\pmb{\beta} + \pmb{\varepsilon}$ 中，设计矩阵 $X$ 存在严重多重共线性。
    -   列出诊断病态性的数值方法，包括条件数、VIF、奇异值分析
    -   比较岭回归、主成分回归和LASSO在处理此问题上的优缺点
    -   设计一个系统流程，从数据检查到模型估计再到结果验证
7.  **高维稀疏回归的算法选择**：考虑高维线性回归 $p \gg n$，假设真实参数 $\pmb{\beta}^*$ 是稀疏的（大多数元素为零）。
    -   解释为什么坐标下降法特别适合求解LASSO问题
    -   推导LASSO的坐标更新公式，并说明软阈值函数的作用
    -   讨论在什么情况下应使用近端梯度法或加速变体（如FISTA）而非坐标下降法
8.  **优化失败案例分析与调试**：分析以下优化失败场景，提出诊断和解决策略：
    -   牛顿法迭代中Hessian矩阵不正定
    -   拟牛顿法收敛到明显错误的解
    -   算法在达到收敛准则前停止，但梯度仍很大
    -   函数值在迭代中不单调下降

### 综合项目

9.  **完整计量模型的数值实现**：选择一个中等复杂的计量模型（如Tobit模型、多层模型或动态面板模型），完成以下任务：
    -   推导对数似然函数、梯度和Hessian矩阵
    -   设计数值实现方案，包括初始值策略、优化算法选择和收敛准则
    -   讨论潜在的数值问题及应对措施
    -   设计模拟实验验证实现的正确性和效率
10. **算法性能比较研究**：对一个具体的计量估计问题（如MLE for Probit模型），设计实验比较不同优化算法的性能：
    -   包括梯度下降法、牛顿法、BFGS、L-BFGS、信任域法和Nelder-Mead法
    -   性能指标：迭代次数、计算时间、最终精度、对初始值的敏感性
    -   在不同问题设置下测试（不同样本量、不同条件数、不同噪声水平）
    -   基于结果给出算法选择的实用建议

这些习题旨在巩固本章的核心概念，并培养将理论知识应用于实际计量问题的能力。理论习题强调数学推导和性质分析，应用习题侧重实践设计和问题解决，综合项目则提供完整的建模与实现体验。

*本章介绍了计量经济学数值计算的核心方法。矩阵分解提供了稳定高效的基础运算，而优化算法则利用这些基础求解复杂的估计问题。理解这两者的原理和相互作用，是进行可靠计量实证研究的关键能力。随着计算技术的发展，这些数值方法将继续演化，但其中蕴含的稳定性、效率和精度权衡的基本原则将始终重要。*